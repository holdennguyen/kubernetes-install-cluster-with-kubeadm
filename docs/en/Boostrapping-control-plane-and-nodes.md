# Bootstrapping control plane and nodes
<p align="left">
<img src="https://kubernetes.io/images/favicon.png" width="50">
</p>

## Initializing your control plane

The `control plane` is the machine where run components including `etcd` (the cluster database) and the `API Server` (which the `kubectl` command line tool communicates with).

To initialize the control plane, run this command in your virtual machine hostname `kubemaster`:

    sudo kubeadm init --apiserver-advertise-address=192.168.56.2 --pod-network-cidr=10.244.0.0/16
 
* `--apiserver-advertise-address=192.168.56.2`: The IP address the API Server will advertise its listening on. In this tutorial, we will use the IP address of `kubemaster` VM.
* `--pod-network-cidr=10.244.0.0/16`: the control plane will automatically allocate CIDRs for every node which specifies a range of IP addresses that can be used for pod IPs. **You may need to choose a CIDR range that does not overlap with any existing network ranges to avoid IP address conflicts**.

`kubeadm init` first runs a series of prechecks to ensure that the machine is ready to run `Kubernetes`. These prechecks expose warnings and exit on errors. `kubeadm init` then downloads and installs the cluster control plane components. This may take several minutes. After it finishes you should see:

    Your Kubernetes control-plane has initialized successfully!

    To start using your cluster, you need to run the following as a regular user:

    mkdir -p $HOME/.kube
    sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
    sudo chown $(id -u):$(id -g) $HOME/.kube/config

    You should now deploy a Pod network to the cluster.
    Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
    /docs/concepts/cluster-administration/addons/

    You can now join any number of machines by running the following on each node
    as root:

    kubeadm join <control-plane-host>:<control-plane-port> --token <token> --discovery-token-ca-cert-hash sha256:<hash>

**Save the command `kubeadm join...` to join the nodes into the cluster later**.

To make `kubectl` work for your `non-root user`, run these commands, which are also part of the `kubeadm init` output:

    mkdir -p $HOME/.kube
    sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
    sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the `root user`, you can run:

    export KUBECONFIG=/etc/kubernetes/admin.conf

>Warning: `Kubeadm` signs the certificate in the `admin.conf` to have `Subject: O = system:masters`, `CN = kubernetes-admin`. 
`system:masters` is a break-glass, super user group that bypasses the authorization layer (e.g. [RBAC](https://docs.oracle.com/cd/E19253-01/816-4557/rbac-1/)). Do not share the `admin.conf` file with anyone and instead grant users custom permissions by generating them a `kubeconfig file` using the `kubeadm kubeconfig` user command. For more details see [Generating kubeconfig files for additional users](https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/#kubeconfig-additional-users).

## Joining your nodes

Run the command that was output by `kubeadm init` on all worker nodes - virtual machine:`kubenode01`, `kubenode02`with sudo permission:

    sudo kubeadm join --token <token> <control-plane-host>:<control-plane-port> --discovery-token-ca-cert-hash sha256:<hash>

**If you lost the output command above, go to the control plane: `kubemaster`.**

**Get the `<token>` by running**

    kubeadm token list

The output is similar to this:

    TOKEN                    TTL  EXPIRES              USAGES           DESCRIPTION            EXTRA GROUPS
    8ewj1p.9r9hcjoqgajrj4gi  23h  2018-06-12T02:51:28Z authentication,  The default bootstrap  system:
                                                       signing          token generated by     bootstrappers:
                                                                        'kubeadm init'.        kubeadm:
                                                                                               default-node-token

By default, `<tokens>` expire after 24 hours. If you are joining a node to the cluster after the current `<token>` has expired, you can create a new `<token>` by running the following command on the `control-plane node`:

    kubeadm token create

The output is similar to this:

    5didvk.d09sbcov8ph2amjw

**Get the `<hash>` by running**

    openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | \
    openssl dgst -sha256 -hex | sed 's/^.* //'

The output is similar to:

    8cb2de97839780a412b93877f8507ad6c94f73add17d5d7058e91741c9d5ec78

**Get the `<control-plane-host>:<control-plane-port>` by running**

    cat /$HOME/.kube/config | grep server

The output is similar to:

    server: https://192.168.56.2:6443

**`<control-plane-host>:<control-plane-port>`** will be **`192.168.56.2:6443`**

**Node join to Kubernetes cluster successfully**

The output should look something like this:

    [preflight] Running pre-flight checks

    ... (log output of join workflow) ...

    Node join complete:
    * Certificate signing request sent to control-plane and response
    received.
    * Kubelet informed of new secure connection details.

    Run 'kubectl get nodes' on control-plane to see this machine join.

A few seconds later, you should notice this node in the output from kubectl get nodes when run on the control-plane node.

## Verify Kubernetes cluster components

![Components of kubernetes](../images/components-of-kubernetes.svg)

On control-plane `kubemaster` and worker nodes `kubenode01`, `kubenode02` run

    sudo netstat -lntp

All components with LISTEN ports will be shown below:

**kubemaster** 

    Active Internet connections (only servers)
    Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
    tcp        0      0 127.0.0.1:10248         0.0.0.0:*               LISTEN      8013/kubelet
    tcp        0      0 127.0.0.1:10249         0.0.0.0:*               LISTEN      8182/kube-proxy
    tcp        0      0 192.168.56.2:2379       0.0.0.0:*               LISTEN      7811/etcd
    tcp        0      0 127.0.0.1:2379          0.0.0.0:*               LISTEN      7811/etcd
    tcp        0      0 192.168.56.2:2380       0.0.0.0:*               LISTEN      7811/etcd
    tcp        0      0 127.0.0.1:2381          0.0.0.0:*               LISTEN      7811/etcd
    tcp        0      0 127.0.0.1:10257         0.0.0.0:*               LISTEN      7791/kube-controlle
    tcp        0      0 127.0.0.1:10259         0.0.0.0:*               LISTEN      7907/kube-scheduler
    tcp        0      0 127.0.0.1:34677         0.0.0.0:*               LISTEN      2826/containerd
    tcp        0      0 127.0.0.53:53           0.0.0.0:*               LISTEN      817/systemd-resolve
    tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1380/sshd
    tcp6       0      0 :::10250                :::*                    LISTEN      8013/kubelet
    tcp6       0      0 :::6443                 :::*                    LISTEN      7884/kube-apiserver
    tcp6       0      0 :::10256                :::*                    LISTEN      8182/kube-proxy
    tcp6       0      0 :::22                   :::*                    LISTEN      1380/sshd

>`kube-apiserver` show that it only LISTEN on IPv6 `:::6443` but actually the API server is listening on an IPv6 address that can be accessed through an `IPv4-mapped IPv6 address`. That why we can run `kubeadm join` on worker nodes succesfully. 
For example, the IPv4 address `192.168.5.2` can be represented as the IPv6 address `::ffff:192.168.5.2`.

**kubenode***

    Active Internet connections (only servers)
    Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name    
    tcp        0      0 127.0.0.1:10248         0.0.0.0:*               LISTEN      8987/kubelet        
    tcp        0      0 127.0.0.1:10249         0.0.0.0:*               LISTEN      9208/kube-proxy     
    tcp        0      0 127.0.0.1:39989         0.0.0.0:*               LISTEN      2785/containerd     
    tcp        0      0 127.0.0.53:53           0.0.0.0:*               LISTEN      782/systemd-resolve 
    tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1431/sshd
    tcp6       0      0 :::10250                :::*                    LISTEN      8987/kubelet        
    tcp6       0      0 :::10256                :::*                    LISTEN      9208/kube-proxy     
    tcp6       0      0 :::22                   :::*                    LISTEN      1431/sshd

## Installing a Pod network add-on

Run `kubectl get nodes` on control-plane to see these joined nodes

    NAME           STATUS     ROLES           AGE    VERSION
    kubemaster     NotReady   control-plane   3h1m   v1.26.2
    kubenode01     NotReady   <none>          3h     v1.26.2
    kubenode02     NotReady   <none>          179m   v1.26.2

As you can see, our virtual machine `kubemaster`, `kubenode01`, `kubenode02` were joined the Kubernetes cluster but they are `NotReady`. 

Run `kubectl get pods -A` on control plane to see all pods of `kube-system`

    NAMESPACE     NAME                                   READY   STATUS    RESTARTS   AGE
    kube-system   coredns-787d4945fb-5cwlq               0/1     Pending   0          3h8m
    kube-system   coredns-787d4945fb-q2s4p               0/1     Pending   0          3h8m
    kube-system   etcd-controlplane                      1/1     Running   0          3h8m
    kube-system   kube-apiserver-controlplane            1/1     Running   0          3h8m
    kube-system   kube-controller-manager-controlplane   1/1     Running   0          3h8m
    kube-system   kube-proxy-7twwr                       1/1     Running   0          3h7m
    kube-system   kube-proxy-8mxt7                       1/1     Running   0          3h8m
    kube-system   kube-proxy-v9rc6                       1/1     Running   0          3h8m
    kube-system   kube-scheduler-controlplane            1/1     Running   0          3h9m

You must deploy a `Container Network Interface (CNI)` based `Pod network add-on` so that your Pods can communicate with each other. `Cluster DNS (CoreDNS)` will not start up until the pod networking is configured.

`Pod network add-ons` are `Kubernetes-specific CNI plugins` that provide **network connectivity between pods** in a `Kubernetes cluster`. They create a `virtual network overlay` that spans the entire cluster and provides each `pod` with its own `unique IP address`.

While `CNI plugins` can be used with any container runtime, pod network add-ons are specific to Kubernetes and provide the networking functionality required for the Kubernetes networking model. Some examples of pod network add-ons include `Calico`, `Flannel`, and `Weave Net`.

In this tutorial, we will use [Weave Net](https://www.weave.works/docs/net/latest/kubernetes/kube-addon/) add-ons. It is easier to set up, and use and is a good fit for smaller-scale deployments.

To install onto the Kubernetes cluster, run the following command on the control plane `kubemaster`:

    kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml

Output will be

    serviceaccount/weave-net created
    clusterrole.rbac.authorization.k8s.io/weave-net created
    clusterrolebinding.rbac.authorization.k8s.io/weave-net created
    role.rbac.authorization.k8s.io/weave-net created
    rolebinding.rbac.authorization.k8s.io/weave-net created
    daemonset.apps/weave-net created

Take care that your Pod network must not overlap with any of the machine networks. If you define a `CIDR block` during `kubeadm init` with `--pod-network-cidr`, insert parameter `IPALLOC_RANGE` in `Weaver network plugin's YAML`.
Run this command on the control plane `kubemaster`:

    kubectl edit ds weave-net -n kube-system

It will open allowing you to edit the YAML file of `weave-net daemon set`. Find `spec` of `container` name `weave` to add an environment variable `IPALLOC_RANGE`, value is `--pod-network-cidr`

    spec:
    ...
        template:
        ...
            spec:
            ...
                containers:
                ...
                    env:
                    - name: IPALLOC_RANGE
                      value: 10.244.0.0/16
                      ...
                    name: weave

Save the file and wait a few minutes for `weave-net` pods rebooting.

## Successful 

Run `kubectl get pods -A` on the control plane again to verify, you will see 3 pods of `weave-net daemon set` and the `coredns` pods are running now:

    NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE
    kube-system   coredns-787d4945fb-48tbh             1/1     Running   0          6m57s
    kube-system   coredns-787d4945fb-nrsp7             1/1     Running   0          6m57s
    kube-system   etcd-kubemaster                      1/1     Running   0          7m10s
    kube-system   kube-apiserver-kubemaster            1/1     Running   0          7m12s
    kube-system   kube-controller-manager-kubemaster   1/1     Running   0          7m10s
    kube-system   kube-proxy-8sxss                     1/1     Running   0          4m19s
    kube-system   kube-proxy-j7z6x                     1/1     Running   0          6m58s
    kube-system   kube-proxy-nj8j2                     1/1     Running   0          4m14s
    kube-system   kube-scheduler-kubemaster            1/1     Running   0          7m10s
    kube-system   weave-net-7mldz                      2/2     Running   0          2m
    kube-system   weave-net-dk5dl                      2/2     Running   0          70s
    kube-system   weave-net-znhnm                      2/2     Running   0          2m

Run `kubectl get nodes` to verify the status of the cluster, all nodes are ready now:

    NAME         STATUS   ROLES           AGE     VERSION
    kubemaster   Ready    control-plane   9m54s   v1.26.2
    kubenode01   Ready    <none>          6m59s   v1.26.2
    kubenode02   Ready    <none>          6m54s   v1.26.2

>If you see that a worker node has `ROLES` of `<none>`, it means that the node is not running any `control plane components` or Kubernetes services that would give it a specific role. Worker nodes typically do not run any control plane components, so this is perfectly normal and expected behavior for worker nodes in a Kubernetes cluster.

Networking is a central part of Kubernetes, see [Kubernetes networking model](https://kubernetes.io/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-networking-model) for more information.

For more options to customize the cluster with kubeadm, read [Create cluster kubeadm](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/).

▶️ [Clean up your environment](Clean-up-environment.md/#clean-up-environment)
